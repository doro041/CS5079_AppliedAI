{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep Q-Network Q(a, s)\n",
    "-----------------------\n",
    "TD Learning, Off-Policy, e-Greedy Exploration (GLIE).\n",
    "Q(S, A) <- Q(S, A) + alpha * (R + lambda * Q(newS, newA) - Q(S, A))\n",
    "delta_w = R + lambda * Q(newS, newA)\n",
    "See David Silver RL Tutorial Lecture 5 - Q-Learning for more details.\n",
    "Reference\n",
    "----------\n",
    "original paper: https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n",
    "EN: https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.5m3361vlw\n",
    "CN: https://zhuanlan.zhihu.com/p/25710327\n",
    "Note: Policy Network has been proved to be better than Q-Learning, see tutorial_atari_pong.py\n",
    "Environment\n",
    "-----------\n",
    "# The FrozenLake v0 environment\n",
    "https://gym.openai.com/envs/FrozenLake-v0\n",
    "The agent controls the movement of a character in a grid world. Some tiles of\n",
    "the grid are walkable, and others lead to the agent falling into the water.\n",
    "Additionally, the movement direction of the agent is uncertain and only partially\n",
    "depends on the chosen direction. The agent is rewarded for finding a walkable\n",
    "path to a goal tile.\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward\n",
    "of 1 if you reach the goal, and zero otherwise.\n",
    "Prerequisites\n",
    "--------------\n",
    "tensorflow>=2.0.0a0\n",
    "tensorlayer>=2.0.0\n",
    "To run\n",
    "-------\n",
    "python tutorial_DQN.py --train/test\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "\n",
    "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self,\n",
    "                  env_id='FrozenLake-v1',\n",
    "                  discount_factor=0.90,\n",
    "                  max_eps=1,\n",
    "                  min_eps=0.1,\n",
    "                  num_episodes=10000,\n",
    "                  eps_decay=0.999993\n",
    "                  ):\n",
    "        self.env_id = env_id\n",
    "        self.discount_factor = discount_factor\n",
    "        self.max_eps = 1  # e-Greedy Exploration, the larger the more random\n",
    "        self.min_eps = 0.1\n",
    "        self.num_episodes = num_episodes\n",
    "        self.render = False  # display the game environment\n",
    "        self.rList = [] #Record reward\n",
    "        self.alg_name = 'DQN'\n",
    "        self.goal_reached_n = 0  # count the number of times the goal is reached\n",
    "        self.eps = self.max_eps\n",
    "        self.eps_decay = eps_decay\n",
    "        self.q_table = np.zeros((100, 4))\n",
    "\n",
    "\n",
    "    ##################### DQN ##########################\n",
    "\n",
    "\n",
    "    def to_one_hot(self, i, n_classes=None):\n",
    "        a = np.zeros(n_classes, 'uint8')\n",
    "        a[i] = 1\n",
    "        return a\n",
    "\n",
    "\n",
    "    ## Define Q-network q(a,s) that ouput the rewards of 4 actions by given state, i.e. Action-Value Function.\n",
    "    # encoding for state: 10x10 grid can be represented by one-hot vector with 100 integers.\n",
    "    def get_model(self, inputs_shape):\n",
    "        ni = tl.layers.Input(inputs_shape, name='observation')\n",
    "        nn = tl.layers.Dense(4, act=None, W_init=tf.random_uniform_initializer(0, 0.01), b_init=None, name='q_a_s')(ni)\n",
    "        return tl.models.Model(inputs=ni, outputs=nn)\n",
    "\n",
    "\n",
    "    def save_ckpt(self, model):  # save trained weights\n",
    "        path = os.path.join('model', '_'.join([self.alg_name, self.env_id]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        tl.files.save_weights_to_hdf5(os.path.join(path, 'dqn_model.hdf5'), model)\n",
    "\n",
    "\n",
    "    def load_ckpt(self, model):  # load trained weights\n",
    "        path = os.path.join('model', '_'.join([self.alg_name, self.env_id]))\n",
    "        tl.files.save_weights_to_hdf5(os.path.join(path, 'dqn_model.hdf5'), model)\n",
    "\n",
    "\n",
    "    def train(self, qnetwork, train_weights, optimizer, env, t0):\n",
    "        flattened_map = env.desc.flatten()\n",
    "        all_episode_reward = []\n",
    "        for i in range(self.num_episodes):\n",
    "            visited = set()\n",
    "            ## Reset environment and get first new observation\n",
    "            s = env.reset()[0]  # observation is state\n",
    "            rAll = 0\n",
    "            if self.render: env.render()\n",
    "            while True:\n",
    "                ## Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                allQ = qnetwork(np.asarray([self.to_one_hot(s, 100)], dtype=np.float32)).numpy()\n",
    "                self.q_table[s] = allQ\n",
    "                a = np.argmax(allQ, 1)\n",
    "\n",
    "                ## e-Greedy Exploration !!! sample random action\n",
    "                if np.random.rand(1) < self.eps:\n",
    "                    a[0] = env.action_space.sample()\n",
    "                ## Get new state and reward from environment\n",
    "                next_state, r, d, _, _ = env.step(a[0])\n",
    "\n",
    "                # if flattened_map[next_state] == b'G':\n",
    "                #     self.goal_reached_n += 1\n",
    "\n",
    "                if next_state in visited:\n",
    "                    r = -10\n",
    "                elif next_state == s and not d:\n",
    "                    r = -10\n",
    "                elif flattened_map[next_state] == b'H':\n",
    "                    r = -30\n",
    "                elif flattened_map[next_state] == b'G':\n",
    "                    r = 100\n",
    "                    self.goal_reached_n += 1\n",
    "                elif flattened_map[next_state] == b'F':\n",
    "                    r = -1\n",
    "\n",
    "                if self.render: env.render()\n",
    "\n",
    "                ## Obtain the Q' values by feeding the new state through our network\n",
    "                Q1 = qnetwork(np.asarray([self.to_one_hot(next_state, 100)], dtype=np.float32)).numpy()\n",
    "                ## Obtain maxQ' and set our target value for chosen action.\n",
    "                maxQ1 = np.max(Q1)  # in Q-Learning, policy is greedy, so we use \"max\" to select the next action.\n",
    "                targetQ = allQ\n",
    "                targetQ[0, a[0]] = r + self.discount_factor * maxQ1\n",
    "                ## Train network using target and predicted Q values\n",
    "                # it is not real target Q value, it is just an estimation,\n",
    "                # but check the Q-Learning update formula:\n",
    "                #    Q'(s,a) <- Q(s,a) + alpha(r + lambd * maxQ(s',a') - Q(s, a))\n",
    "                # minimizing |r + lambd * maxQ(s',a') - Q(s, a)|^2 equals to force Q'(s,a) â‰ˆ Q(s,a)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    _qvalues = qnetwork(np.asarray([self.to_one_hot(s, 100)], dtype=np.float32))\n",
    "                    _loss = tl.cost.mean_squared_error(targetQ, _qvalues, is_mean=False)\n",
    "                grad = tape.gradient(_loss, train_weights)\n",
    "                optimizer.apply_gradients(zip(grad, train_weights))\n",
    "\n",
    "                rAll += r\n",
    "                s = next_state\n",
    "                \n",
    "                if s not in visited:\n",
    "                    visited.add(s)\n",
    "                ## Reduce chance of random action if an episode is done.\n",
    "                if d == True:\n",
    "                    if self.eps > self.min_eps:\n",
    "                        self.eps *= self.eps_decay\n",
    "                    break\n",
    "\n",
    "            ## Note that, the rewards here with random action\n",
    "            if i % 10000 == 0:\n",
    "                print('Training  | Episode: {}/{}  | Episode Reward: {:.4f} | Epsilon: {:.4f} | Goal reached: {}' \\\n",
    "                        .format(i, self.num_episodes, rAll, self.eps, self.goal_reached_n))\n",
    "            if i % 100000 == 0 :\n",
    "                print(self.q_table)\n",
    "            if i == 0:\n",
    "                all_episode_reward.append(rAll)\n",
    "            else:\n",
    "                all_episode_reward.append(all_episode_reward[-1] * 0.9 + rAll * 0.1)\n",
    "\n",
    "        self.save_ckpt(qnetwork)  # save model\n",
    "        print(self.q_table)\n",
    "        plt.plot(all_episode_reward)\n",
    "        if not os.path.exists('image'):\n",
    "            os.makedirs('image')\n",
    "        plt.savefig(os.path.join('image', '_'.join([self.alg_name, self.env_id])))\n",
    "\n",
    "    \n",
    "    # def test(self, qnetwork, num_episodes, env, t0):\n",
    "    #     self.load_ckpt(qnetwork)  # load model\n",
    "    #     for i in range(num_episodes):\n",
    "    #         ## Reset environment and get first new observation\n",
    "    #         s = env.reset()[0]  # observation is state, integer 0 ~ 15\n",
    "    #         rAll = 0\n",
    "    #         if self.render: env.render()\n",
    "    #         for j in range(99):  # step index, maximum step is 99\n",
    "    #             ## Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "    #             allQ = qnetwork(np.asarray([self.to_one_hot(s, 16)], dtype=np.float32)).numpy()\n",
    "    #             a = np.argmax(allQ, 1)  # no epsilon, only greedy for testing\n",
    "\n",
    "    #             ## Get new state and reward from environment\n",
    "    #             s1, r, d, _ = env.step(a[0])\n",
    "    #             rAll += r\n",
    "    #             s = s1\n",
    "    #             if self.render: env.render()\n",
    "    #             ## Reduce chance of random action if an episode is done.\n",
    "    #             if d: break\n",
    "\n",
    "    #         print('Testing  | Episode: {}/{}  | Episode Reward: {:.4f} | Running Time: {:.4f}' \\\n",
    "    #               .format(i, num_episodes, rAll, time.time() - t0))\n",
    "    #         self.rList.append(rAll)\n",
    "    #     print(\"Correct rate: \" + str(sum(self.rList) / num_episodes * 100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "\n",
    "agent = DQNAgent(num_episodes=1_000_000, eps_decay=0.9999975)\n",
    "\n",
    "qnetwork = agent.get_model([None, 100])\n",
    "qnetwork.train()\n",
    "train_weights = qnetwork.trainable_weights\n",
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
    "random_map = generate_random_map(size=10, p=0.3)    \n",
    "\n",
    "env = gym.make(agent.env_id, desc=random_map,render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "\n",
    "t0 = time.time()\n",
    "plt.imshow(env.render())\n",
    "display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(qnetwork, train_weights, optimizer, env, t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.goal_reached_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_ckpt(qnetwork)  # load model\n",
    "for i in range(agent.num_episodes):\n",
    "    ## Reset environment and get first new observation\n",
    "    s = env.reset()[0]  # observation is state, integer 0 ~ 15\n",
    "    rAll = 0\n",
    "    if agent.render: env.render()\n",
    "    for j in range(99):  # step index, maximum step is 99\n",
    "        ## Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "        allQ = qnetwork(np.asarray([agent.to_one_hot(s, 100)], dtype=np.float32)).numpy()\n",
    "        a = np.argmax(allQ, 1)  # no epsilon, only greedy for testing\n",
    "\n",
    "        ## Get new state and reward from environment\n",
    "        s1, r, d, _, _ = env.step(a[0])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if agent.render: env.render()\n",
    "        ## Reduce chance of random action if an episode is done.\n",
    "        if d: break\n",
    "\n",
    "    print('Testing  | Episode: {}/{}  | Episode Reward: {:.4f} | Running Time: {:.4f}' \\\n",
    "            .format(i, agent.num_episodes, rAll, time.time() - t0))\n",
    "    agent.rList.append(rAll)\n",
    "print(\"Correct rate: \" + str(sum(agent.rList) / agent.num_episodes * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnetwork.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
